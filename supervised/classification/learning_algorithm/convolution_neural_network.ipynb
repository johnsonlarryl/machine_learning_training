{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69422b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d8f8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 15, 3, 1, 1)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc0d1d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 3, 3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the weights to be learned\n",
    "conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95358c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Boat_in_the_beach_Chacachacare.jpg/640px-Boat_in_the_beach_Chacachacare.jpg\", \"boat.png\")\n",
    "img = Image.open(\"boat.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c9138b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378, 640, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the img as a numpy array to see the values at individual pixels\n",
    "# Note that the structure of the image we loaded puts the channel \n",
    "# information at the innermost dimension â€“ which is different \n",
    "# when we load the image in Torch.\n",
    "np.array(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7b168d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c88b3c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 378)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e97d585f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 378, 640])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = TF.to_tensor(img)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4deb9bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.7373, 0.7373, 0.7333,  ..., 0.8549, 0.8549, 0.8549],\n",
       "          [0.7333, 0.7333, 0.7333,  ..., 0.8627, 0.8588, 0.8549],\n",
       "          [0.7333, 0.7294, 0.7294,  ..., 0.8627, 0.8588, 0.8549],\n",
       "          ...,\n",
       "          [0.8000, 0.8039, 0.7961,  ..., 0.8627, 0.7922, 0.8275],\n",
       "          [0.7961, 0.8196, 0.8549,  ..., 0.7804, 0.7020, 0.6863],\n",
       "          [0.8627, 0.8471, 0.8392,  ..., 0.7451, 0.7059, 0.7333]],\n",
       "\n",
       "         [[0.8431, 0.8431, 0.8392,  ..., 0.9176, 0.9176, 0.9176],\n",
       "          [0.8392, 0.8392, 0.8392,  ..., 0.9255, 0.9216, 0.9176],\n",
       "          [0.8392, 0.8353, 0.8353,  ..., 0.9255, 0.9216, 0.9176],\n",
       "          ...,\n",
       "          [0.7176, 0.7137, 0.6941,  ..., 0.7804, 0.7098, 0.7451],\n",
       "          [0.7137, 0.7294, 0.7569,  ..., 0.6980, 0.6196, 0.6039],\n",
       "          [0.7804, 0.7569, 0.7412,  ..., 0.6627, 0.6235, 0.6510]],\n",
       "\n",
       "         [[0.9490, 0.9490, 0.9451,  ..., 0.9765, 0.9765, 0.9765],\n",
       "          [0.9451, 0.9451, 0.9451,  ..., 0.9843, 0.9804, 0.9765],\n",
       "          [0.9451, 0.9412, 0.9412,  ..., 0.9843, 0.9804, 0.9765],\n",
       "          ...,\n",
       "          [0.6510, 0.6510, 0.6275,  ..., 0.6980, 0.6275, 0.6627],\n",
       "          [0.6471, 0.6667, 0.6784,  ..., 0.6157, 0.5373, 0.5216],\n",
       "          [0.7137, 0.6863, 0.6627,  ..., 0.5804, 0.5412, 0.5686]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0436bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 378, 640])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebef6e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a simple filter that will boost the contrast sharply.\n",
    "img_filter = torch.zeros((1,3,3,3))\n",
    "img_filter[0, :] = torch.tensor([[-10, 10, -10], [10, 100, 10], [-10, 10, 10]])\n",
    "img_filter = torch.nn.Parameter(img_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb56e3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-10.,  10., -10.],\n",
       "          [ 10., 100.,  10.],\n",
       "          [-10.,  10.,  10.]],\n",
       "\n",
       "         [[-10.,  10., -10.],\n",
       "          [ 10., 100.,  10.],\n",
       "          [-10.,  10.,  10.]],\n",
       "\n",
       "         [[-10.,  10., -10.],\n",
       "          [ 10., 100.,  10.],\n",
       "          [-10.,  10.,  10.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc9494fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z is originally created as a 1x1x378x640 array, of which we extract\n",
    "# the actual image\n",
    "# This produces a sharp highlighted grayscale image\n",
    "z = F.conv2d(x, img_filter, padding=1, stride=1)\n",
    "z = z.detach().numpy()[0][0]\n",
    "contrast_img = Image.fromarray(z)\n",
    "contrast_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0f7afb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 378)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c4798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
